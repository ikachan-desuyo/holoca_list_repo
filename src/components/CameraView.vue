<template>
  <div>
    <button v-if="!cameraActive && opencvReady" @click="startCamera">ğŸ“¸ ã‚«ãƒ¡ãƒ©ã‚’èµ·å‹•</button>
    <video ref="videoRef" autoplay muted playsinline style="display:none"></video>
    <canvas
      ref="canvasRef"
      style="display:block; margin-top:12px; width:100%; max-width:400px; border:1px solid #ccc;"
    ></canvas>
    <div v-if="!opencvReady" style="color:#f00; margin-top:12px;">OpenCV.js èª­ã¿è¾¼ã¿ä¸­...</div>
    <div v-if="errorMsg" style="color:#f00; margin-top:12px; white-space:pre-wrap;">{{ errorMsg }}</div>
    <div v-if="recognizedCard" style="color:#080; margin-top:12px;">èªè­˜çµæœ: {{ recognizedCard }}</div>
    <button v-if="cameraActive && opencvReady" @click="onRecognize">ã‚«ãƒ¼ãƒ‰èªè­˜ã™ã‚‹</button>
    <!-- â–¼ ç‰¹å¾´ç‚¹å¯è¦–åŒ–ã‚»ãƒ¬ã‚¯ã‚¿ã¨ç”»åƒè¡¨ç¤ºã‚’è¿½åŠ  -->
    <div style="margin-top:24px;">
      <label>ç‰¹å¾´ç‚¹å¯è¦–åŒ–ã‚«ãƒ¼ãƒ‰é¸æŠï¼š</label>
      <select v-model="selectedFeatureIdx">
        <option v-for="(feature, idx) in features" :key="feature.image_url" :value="idx">
          {{ feature.image_url ? feature.image_url.split('/').pop() : feature.name || idx }}
        </option>
      </select>
      <div v-if="selectedFeatureIdx !== null && features[selectedFeatureIdx]">
        <img
          :src="features[selectedFeatureIdx].image_url"
          :alt="features[selectedFeatureIdx].name"
          ref="featureImgRef"
          style="max-width:300px; display:block; margin:12px 0; border:1px solid #ccc;"
          @load="drawKeypoints"
        />
        <canvas
          ref="featureKeyCanvasRef"
          style="max-width:300px; display:block; margin-bottom:12px; border:1px solid #ccc;"
        ></canvas>
      </div>
    </div>
    <!-- â–² ã“ã“ã¾ã§ -->
  </div>
</template>

<script setup lang="ts">
import { ref, onMounted, watch } from 'vue'

const videoRef = ref<HTMLVideoElement>()
const canvasRef = ref<HTMLCanvasElement>()
const cameraActive = ref(false)
const opencvReady = ref(false)
const errorMsg = ref('')
const features = ref<any[]>([])
const recognizedCard = ref('')
let stream: MediaStream | null = null

// â–¼ ç‰¹å¾´ç‚¹å¯è¦–åŒ–ç”¨
const selectedFeatureIdx = ref<number | null>(null)
const featureImgRef = ref<HTMLImageElement>()
const featureKeyCanvasRef = ref<HTMLCanvasElement>()
// â–²

onMounted(async () => {
  function checkOpenCV() {
    if (window.cv && cv.Mat) {
      opencvReady.value = true
    } else {
      opencvReady.value = false
    }
  }
  const scripts = document.getElementsByTagName('script')
  for (let i = 0; i < scripts.length; i++) {
    const s = scripts[i]
    if (s.src && s.src.includes('opencv.js')) {
      s.addEventListener('load', checkOpenCV)
    }
  }
  setTimeout(checkOpenCV, 2000)
  checkOpenCV()

  // features.jsonã®èª­ã¿è¾¼ã¿ï¼ˆVite/GitHub Pageså¯¾å¿œï¼‰
  try {
    const baseUrl = import.meta.env.BASE_URL || '/'
    let idx = 1
    let allFeatures: any[] = []
    while (true) {
      const url = `${baseUrl}features${idx}.json`
      try {
        const res = await fetch(url)
        if (!res.ok) break
        const data = await res.json()
        if (!Array.isArray(data) || data.length === 0) break
        allFeatures = allFeatures.concat(data)
        idx++
      } catch {
        break
      }
    }
    features.value = allFeatures
    if (features.value.length > 0) selectedFeatureIdx.value = 0
  } catch (e) {
    errorMsg.value = 'features*.jsonã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ\n' + (e instanceof Error ? e.message : String(e))
  }
})

function onRecognize() {
  if (!cameraActive.value || !opencvReady.value || !canvasRef.value) return
  const canvas = canvasRef.value
  let gray: any
  try {
    const src = cv.imread(canvas)
    gray = new cv.Mat()
    cv.cvtColor(src, gray, cv.COLOR_RGBA2GRAY)
    src.delete()
  } catch (e) {
    errorMsg.value = 'èªè­˜ç”¨ç”»åƒå–å¾—å¤±æ•—\n' + (e instanceof Error ? e.message : String(e))
    return
  }
  recognizeCard(gray)
  gray.delete()
}

// â–¼ ç‰¹å¾´ç‚¹å¯è¦–åŒ–å‡¦ç†
function drawKeypoints() {
  const idx = selectedFeatureIdx.value
  if (idx === null || !features.value[idx] || !featureImgRef.value || !featureKeyCanvasRef.value) return
  const img = featureImgRef.value
  const canvas = featureKeyCanvasRef.value
  const ctx = canvas.getContext('2d')!
  // ç”»åƒã‚µã‚¤ã‚ºã«canvasã‚’åˆã‚ã›ã‚‹
  canvas.width = img.naturalWidth
  canvas.height = img.naturalHeight
  ctx.clearRect(0, 0, canvas.width, canvas.height)
  ctx.drawImage(img, 0, 0, canvas.width, canvas.height)
  // ç‰¹å¾´ç‚¹ã‚’æç”»
  const keypoints = features.value[idx].keypoints || []
  ctx.save()
  ctx.strokeStyle = 'red'
  ctx.lineWidth = 2
  for (const kp of keypoints) {
    // ORBã®å ´åˆ [x, y, size, angle, response, octave, class_id] ãªã©
    const x = kp[0], y = kp[1]
    ctx.beginPath()
    ctx.arc(x, y, 4, 0, 2 * Math.PI)
    ctx.stroke()
  }
  ctx.restore()
}
// ã‚»ãƒ¬ã‚¯ãƒˆå¤‰æ›´æ™‚ã‚„ç”»åƒãƒ­ãƒ¼ãƒ‰æ™‚ã«å†æç”»
watch(selectedFeatureIdx, () => setTimeout(drawKeypoints, 0))
// â–²

async function startCamera() {
  errorMsg.value = ''
  recognizedCard.value = ''
  if (stream) {
    stream.getTracks().forEach(track => track.stop())
  }
  try {
    stream = await navigator.mediaDevices.getUserMedia({
      video: { facingMode: "environment" }
    })
    if (videoRef.value) {
      videoRef.value.srcObject = stream
      await videoRef.value.play()
      cameraActive.value = true
      startDrawingLoop()
    }
  } catch (err) {
    try {
      stream = await navigator.mediaDevices.getUserMedia({ video: true })
      if (videoRef.value) {
        videoRef.value.srcObject = stream
        await videoRef.value.play()
        cameraActive.value = true
        startDrawingLoop()
      }
    } catch (err2) {
      errorMsg.value = 'ã‚«ãƒ¡ãƒ©ãŒèµ·å‹•ã§ãã¾ã›ã‚“ã§ã—ãŸ\n' + (err2 instanceof Error ? err2.message : String(err2))
    }
  }
}

function startDrawingLoop() {
  const video = videoRef.value!
  const canvas = canvasRef.value!
  const ctx = canvas.getContext('2d')!

  function waitForVideoReady() {
    if (video.videoWidth > 0 && video.videoHeight > 0) {
      canvas.width = video.videoWidth
      canvas.height = video.videoHeight
      loop()
    } else {
      requestAnimationFrame(waitForVideoReady)
    }
  }

  function loop() {
    ctx.clearRect(0, 0, canvas.width, canvas.height)
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height)

    try {
      if (window.cv && cv.Mat) {
        // --- OpenCVç”»åƒå‡¦ç† ---
        let src: any
        try {
          src = cv.imread(canvas)
          if (!src || src.empty()) {
            throw new Error('cv.imread(canvas) ã§ç”»åƒã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ')
          }
        } catch (e) {
          errorMsg.value = 'cv.imread(canvas) ã§å¤±æ•—ã—ã¾ã—ãŸ\n' + (e instanceof Error ? e.message : String(e))
          requestAnimationFrame(loop)
          return
        }

        const gray = new cv.Mat()
        const edges = new cv.Mat()
        try {
          cv.cvtColor(src, gray, cv.COLOR_RGBA2GRAY)
          cv.Canny(gray, edges, 50, 150)
        } catch (e) {
          errorMsg.value = 'OpenCVç”»åƒå‡¦ç†ã§å¤±æ•—ã—ã¾ã—ãŸ\n' + (e instanceof Error ? e.message : String(e))
          src.delete()
          gray.delete()
          edges.delete()
          requestAnimationFrame(loop)
          return
        }

        // ã‚¨ãƒƒã‚¸ç”»åƒã‚’RGBAã«å¤‰æ›ã—ã¦é‡ã­æç”»
        try {
          const rgba = new cv.Mat()
          cv.cvtColor(edges, rgba, cv.COLOR_GRAY2RGBA)
          const edgeImageData = new ImageData(
            new Uint8ClampedArray(rgba.data),
            rgba.cols,
            rgba.rows
          )
          ctx.save()
          ctx.globalAlpha = 0.5
          ctx.putImageData(edgeImageData, 0, 0)
          ctx.restore()
          rgba.delete()
        } catch (e) {
          errorMsg.value = 'ImageDataç”Ÿæˆã¾ãŸã¯æç”»ã§å¤±æ•—ã—ã¾ã—ãŸ\n' + (e instanceof Error ? e.message : String(e))
        }

        src.delete()
        gray.delete()
        edges.delete()
      } else {
        ctx.font = 'bold 32px sans-serif'
        ctx.fillStyle = '#f00'
        ctx.fillText('OpenCV loading...', 30, 50)
      }
    } catch (e) {
      errorMsg.value = 'OpenCV error!\n' + (e instanceof Error ? e.message : String(e))
    }

    requestAnimationFrame(loop)
  }

  waitForVideoReady()
}

// ã‚«ãƒ¼ãƒ‰èªè­˜å‡¦ç†
function recognizeCard(gray: any) {
  // ORBç‰¹å¾´é‡æŠ½å‡º
  const orb = new cv.ORB()
  const keypoints = new cv.KeyPointVector()
  const descriptors = new cv.Mat()
  orb.detectAndCompute(gray, new cv.Mat(), keypoints, descriptors)

  let bestMatchIdx = -1
  let bestMatchCount = 0

  for (let idx = 0; idx < features.value.length; idx++) {
    const feature = features.value[idx]
    // DBå´ã®descriptorsã‚’Matã«å¤‰æ›
    const dbDescArr = feature.descriptors.flat()
    const dbDescriptors = cv.matFromArray(
      feature.descriptors.length,
      feature.descriptors[0].length,
      cv.CV_8U,
      dbDescArr
    )
    const bf = new cv.BFMatcher(cv.NORM_HAMMING, true)
    const matches = new cv.DMatchVector()
    bf.match(descriptors, dbDescriptors, matches)
    if (matches.size() > bestMatchCount) {
      bestMatchCount = matches.size()
      bestMatchIdx = idx
    }
    dbDescriptors.delete()
    matches.delete()
    bf.delete()
  }

  descriptors.delete()
  keypoints.delete()
  orb.delete()

  if (bestMatchIdx >= 0 && bestMatchCount > 10) { // ãƒãƒƒãƒæ•°é–¾å€¤ã¯é©å®œèª¿æ•´
    recognizedCard.value = features.value[bestMatchIdx].name + `ï¼ˆãƒãƒƒãƒæ•°: ${bestMatchCount}ï¼‰`
  } else {
    recognizedCard.value = 'èªè­˜ã§ãã¾ã›ã‚“ã§ã—ãŸ'
  }
}
</script>